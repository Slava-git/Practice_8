{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "zRK4zSYae575"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime\n",
        "!pip install onnx"
      ],
      "metadata": {
        "id": "EliUkwwKoOt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmODD-3iNpB4",
        "outputId": "c3e79e91-3539-45ac-eed6-5a68321bbd4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset\n",
        "import torch.optim as optim\n",
        "\n",
        "from transformers import XLMRobertaForTokenClassification, AutoConfig\n",
        "\n",
        "from torch.nn import KLDivLoss\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "Lq4aDAwGOK5N"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ukraine = pd.read_csv('/content/drive/MyDrive/iasa_nlp/uk_geo_dataset.csv')\n",
        "\n",
        "df_ukraine.drop_duplicates(inplace = True)"
      ],
      "metadata": {
        "id": "hynuxQSYRl01"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('ukr-models/uk-ner')\n",
        "\n",
        "# fine-tuned XLMRoberta on pretty small sample of uk_geo_dataset\n",
        "model_ukr = torch.load('/content/drive/MyDrive/iasa_nlp/ukr_model.pt', map_location=torch.device('cpu'))"
      ],
      "metadata": {
        "id": "CfiZzi0xP9RB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# text processing"
      ],
      "metadata": {
        "id": "pPXrezRRn37x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_iob_tags(text, locs, orgs, pers):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokenized_text = tokenizer.encode(text, add_special_tokens=False)\n",
        "    iob_tags = ['O'] * len(tokens)\n",
        "\n",
        "    locs = eval(locs)\n",
        "    orgs = eval(orgs)\n",
        "    pers = eval(pers)\n",
        "\n",
        "    def update_tags(tags, markers, label):\n",
        "        for marker in markers:\n",
        "            start_char, end_char = marker\n",
        "            start_token, end_token = None, None\n",
        "\n",
        "            char_count = 0\n",
        "            found_start = False\n",
        "            for i, token in enumerate(tokens):\n",
        "\n",
        "                if char_count >= start_char and not found_start:\n",
        "                    start_token = i\n",
        "                    found_start = True\n",
        "\n",
        "                char_count += len(token)\n",
        "                if char_count >= end_char:\n",
        "                    end_token = i\n",
        "                    break\n",
        "\n",
        "            # This handles situations where markers might not align with token boundaries.\n",
        "            if start_token is None or end_token is None:\n",
        "                continue\n",
        "\n",
        "            # Check if the start_token is the space token\n",
        "            if tokens[start_token] == '‚ñÅ' and (start_token+1) < len(tokens):\n",
        "                tags[start_token+1] = \"B-\" + label\n",
        "                for j in range(start_token+2, end_token+1):\n",
        "                    tags[j] = \"I-\" + label\n",
        "            else:\n",
        "                tags[start_token] = \"B-\" + label\n",
        "                for j in range(start_token+1, end_token+1):\n",
        "                    tags[j] = \"I-\" + label\n",
        "\n",
        "    update_tags(iob_tags, locs, \"LOC\")\n",
        "    update_tags(iob_tags, orgs, \"ORG\")\n",
        "    update_tags(iob_tags, pers, \"PER\")\n",
        "\n",
        "    return iob_tags"
      ],
      "metadata": {
        "id": "YSt3WfDYNba0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_eval = df_ukraine.iloc[500:600]"
      ],
      "metadata": {
        "id": "QIvFF9nwHTJr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ukraine = df_ukraine.iloc[:500]"
      ],
      "metadata": {
        "id": "QR5QDMc3Nz7W"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating rags based on the labels given in the dataset"
      ],
      "metadata": {
        "id": "lrgZECBGqXv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_ukraine['IOB_tags'] = df_ukraine.apply(lambda row: get_iob_tags(row['text'], row['loc_markers'], row['org_markers'], row['per_markers']), axis=1)"
      ],
      "metadata": {
        "id": "paQblHKrMGDi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_eval['IOB_tags'] = df_eval.apply(lambda row: get_iob_tags(row['text'], row['loc_markers'], row['org_markers'], row['per_markers']), axis=1)"
      ],
      "metadata": {
        "id": "Ox4RvSdZciWz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizing my text"
      ],
      "metadata": {
        "id": "nPiyY_oUe1M6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ukranian\n",
        "uk_train_tokenized_text = df_ukraine['text'].apply(lambda x: tokenizer.tokenize(x)).tolist()\n",
        "uk_train_labels =  df_ukraine['IOB_tags'].tolist()"
      ],
      "metadata": {
        "id": "KYWKTu7KmqwE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eval\n",
        "uk_eval_tokenized_texts = df_eval['text'].apply(lambda x: tokenizer.tokenize(x)).tolist()\n",
        "uk_eval_true_labels =  df_eval['IOB_tags'].tolist()"
      ],
      "metadata": {
        "id": "ZuOniGDn1k4O"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset creation"
      ],
      "metadata": {
        "id": "g2VFbVGqe8W8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapiing tags\n",
        "config = AutoConfig.from_pretrained(\"ukr-models/uk-ner\")\n",
        "\n",
        "text_label_to_model_label = {\n",
        "    \"O\": \"LABEL_0\",\n",
        "    \"B-PER\": \"LABEL_1\",\n",
        "    \"I-PER\": \"LABEL_2\",\n",
        "    \"B-ORG\": \"LABEL_3\",\n",
        "    \"I-ORG\": \"LABEL_4\",\n",
        "    \"B-LOC\": \"LABEL_5\",\n",
        "    \"I-LOC\": \"LABEL_6\"\n",
        "}\n",
        "\n",
        "label2idx = config.label2id\n",
        "\n",
        "tag2idx = {text_label: label2idx[model_label] for text_label, model_label in text_label_to_model_label.items()}\n",
        "\n",
        "tag2idx[\"-100\"] = -100\n"
      ],
      "metadata": {
        "id": "HXPomnrqn5a3"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NERDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert tokens and tags to their respective IDs\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(text)\n",
        "        tag_ids = [tag2idx.get(l) for l in label]\n",
        "\n",
        "        # Create attention mask (1 for real tokens, 0 for padding)\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
        "            'tag_ids': torch.tensor(tag_ids, dtype=torch.long)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "twtB6S0Ue6LZ"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def pad_collate_fn(batch):\n",
        "    all_input_ids = [item['input_ids'] for item in batch]\n",
        "    all_attention_masks = [item['attention_mask'] for item in batch]\n",
        "    all_tag_ids = [item['tag_ids'] for item in batch]\n",
        "\n",
        "    # Pad the sequences\n",
        "    padded_input_ids = pad_sequence(all_input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    padded_attention_masks = pad_sequence(all_attention_masks, batch_first=True, padding_value=0)\n",
        "    padded_tag_ids = pad_sequence(all_tag_ids, batch_first=True, padding_value=tag2idx[\"-100\"])\n",
        "\n",
        "    return {\n",
        "        'input_ids': padded_input_ids,\n",
        "        'attention_mask': padded_attention_masks,\n",
        "        'tag_ids': padded_tag_ids\n",
        "    }\n"
      ],
      "metadata": {
        "id": "ZufN5zupfVz2"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = NERDataset(uk_train_tokenized_text, uk_train_labels)\n",
        "val_dataset = NERDataset(uk_eval_tokenized_texts, uk_eval_true_labels)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=pad_collate_fn)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=16, collate_fn=pad_collate_fn)"
      ],
      "metadata": {
        "id": "p7jseOu0hJgo"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model optimization"
      ],
      "metadata": {
        "id": "gTHrjGHuasq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the teacher model\n",
        "teacher_model = model_ukr"
      ],
      "metadata": {
        "id": "8U6MFr7VavP5"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the student model that is half the size the original  "
      ],
      "metadata": {
        "id": "stx3Slw2o3Zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_student_model(teacher_model, teacher_config):\n",
        "\n",
        "    student_config = teacher_config.num_hidden_layers//2\n",
        "\n",
        "    student_model = AutoModelForTokenClassification(student_config)\n",
        "\n",
        "    # Distill weights from teacher to student\n",
        "    distill_xlm_roberta_weights(teacher=teacher_model.roberta, student=student_model.roberta)\n",
        "\n",
        "    return student_model"
      ],
      "metadata": {
        "id": "-LwDEvUaAjgH"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_student_model(teacher_model):\n",
        "    # Load and modify the configuration from the teacher model\n",
        "    teacher_config = teacher_model.config\n",
        "    student_config = AutoConfig.from_pretrained(teacher_model.config.name_or_path)\n",
        "    student_config.num_hidden_layers //= 2\n",
        "\n",
        "    # Create the student model using the modified configuration\n",
        "    student_model = AutoModelForTokenClassification.from_config(student_config)\n",
        "\n",
        "    # Distill weights from teacher to student\n",
        "    distill_xlm_roberta_weights(teacher=teacher_model, student=student_model)\n",
        "\n",
        "    return student_model\n",
        "\n",
        "def distill_xlm_roberta_weights(teacher, student):\n",
        "    if isinstance(teacher, XLMRobertaForTokenClassification) and isinstance(student, XLMRobertaForTokenClassification):\n",
        "        teacher_encoder = teacher.roberta\n",
        "        student_encoder = student.roberta\n",
        "        distill_encoder_weights(teacher_encoder, student_encoder)\n",
        "        student.classifier.load_state_dict(teacher.classifier.state_dict())\n",
        "\n",
        "def distill_encoder_weights(teacher, student):\n",
        "    teacher_layers = list(teacher.encoder.layer)\n",
        "    student_layers = list(student.encoder.layer)\n",
        "    for i in range(len(student_layers)):\n",
        "        student_layers[i].load_state_dict(teacher_layers[2 * i].state_dict())\n"
      ],
      "metadata": {
        "id": "hjBhhFYkAtS2"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "student_model = create_student_model(teacher_model)"
      ],
      "metadata": {
        "id": "hna-3geX_48z"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "EPOCHS = 3\n",
        "optimizer = optim.AdamW(teacher_model.parameters(), lr=5e-5)"
      ],
      "metadata": {
        "id": "ACc6O-q4hUxv"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " training a smaller \"student\" model to get the weights of a larger \"teacher\" model."
      ],
      "metadata": {
        "id": "7sgbPr7xqE7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_model = teacher_model.to(device)\n",
        "teacher_model.eval()  # Teacher model is always in evaluation\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=tag2idx[\"-100\"]).to(device)\n",
        "\n",
        "temperature = 2.0  # Temperature for softmax\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    student_model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "        # Get input and target tensors from the batch and move them to the device\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        tag_ids = batch['tag_ids'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        student_outputs = student_model(input_ids, attention_mask=attention_mask).logits\n",
        "        student_logits = student_outputs.view(-1, student_outputs.shape[-1])\n",
        "\n",
        "        # Forward pass for the teacher model\n",
        "        with torch.no_grad():\n",
        "            teacher_outputs = teacher_model(input_ids, attention_mask=attention_mask).logits\n",
        "            teacher_logits = teacher_outputs.view(-1, teacher_outputs.shape[-1])\n",
        "\n",
        "        # Soften the logits and calculate the distillation loss\n",
        "        loss_soft = KLDivLoss(reduction='batchmean')(F.log_softmax(student_logits / temperature, dim=-1),\n",
        "                                                     F.softmax(teacher_logits / temperature, dim=-1))\n",
        "\n",
        "        # Calculate the hard loss, which is the usual Cross Entropy loss with true labels\n",
        "        loss_hard = criterion(student_logits, tag_ids.view(-1))\n",
        "\n",
        "        loss = loss_soft + loss_hard\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i + 1) % 10 == 0:  # Print loss every 10 batches\n",
        "            print(f\"Epoch [{epoch + 1}/{EPOCHS}], Step [{i + 1}/{len(train_dataloader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Average training loss for epoch {epoch + 1}: {avg_train_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnNQfwzN4mm8",
        "outputId": "93327ce2-cb4b-4d21-90ea-211a97b027e0"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/3], Step [10/32], Loss: 1.9676\n",
            "Epoch [1/3], Step [20/32], Loss: 1.8149\n",
            "Epoch [1/3], Step [30/32], Loss: 1.9854\n",
            "Average training loss for epoch 1: 1.9635\n",
            "Epoch [2/3], Step [10/32], Loss: 1.9393\n",
            "Epoch [2/3], Step [20/32], Loss: 1.9928\n",
            "Epoch [2/3], Step [30/32], Loss: 1.9743\n",
            "Average training loss for epoch 2: 1.9586\n",
            "Epoch [3/3], Step [10/32], Loss: 2.0235\n",
            "Epoch [3/3], Step [20/32], Loss: 2.0700\n",
            "Epoch [3/3], Step [30/32], Loss: 1.9686\n",
            "Average training loss for epoch 3: 1.9592\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting model to ONNX format"
      ],
      "metadata": {
        "id": "r7JZFkcIqQgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = df_ukraine.iloc[0].text\n",
        "inputs = tokenizer(example_text, return_tensors=\"pt\")\n",
        "\n",
        "dummy_input = inputs[\"input_ids\"]\n",
        "\n",
        "student_model.eval()\n",
        "student_model.cpu()\n",
        "\n",
        "# Export the model\n",
        "output_onnx_file = \"/content/drive/MyDrive/iasa_nlp/student_model.onnx\"\n",
        "torch.onnx.export(student_model,\n",
        "                  dummy_input,\n",
        "                  output_onnx_file,\n",
        "                  export_params=True,\n",
        "                  opset_version=11,\n",
        "                  do_constant_folding=True,\n",
        "                  input_names=['input_ids'],\n",
        "                  output_names=['output'],\n",
        "                  dynamic_axes={'input_ids': {0: 'batch_size', 1: 'sequence'},\n",
        "                                'output': {0: 'batch_size', 1: 'sequence'}})"
      ],
      "metadata": {
        "id": "i_vmARu7iWZQ"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/iasa_nlp/student_model.onnx\"\n",
        "quantized_model_path = \"/content/drive/MyDrive/iasa_nlp/student_model_quantized.onnx\"\n",
        "\n",
        "\n",
        "quantize_dynamic(model_path,\n",
        "                 quantized_model_path,\n",
        "                 weight_type=QuantType.QUInt8)\n"
      ],
      "metadata": {
        "id": "VUc25IDtlLXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "YANnygmW5bgU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation for teacher model"
      ],
      "metadata": {
        "id": "wYUr3fOW5mdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "teacher_model.eval()\n",
        "teacher_predictions = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['tag_ids'].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = teacher_model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        teacher_predictions.extend(predictions.view(-1).cpu().numpy())\n",
        "        true_labels.extend(labels.view(-1).cpu().numpy())\n",
        "\n",
        "teacher_predictions = [pred for pred, label in zip(teacher_predictions, true_labels) if label != -100]\n",
        "true_labels = [label for label in true_labels if label != -100]\n",
        "\n",
        "\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, teacher_predictions, average='weighted')\n"
      ],
      "metadata": {
        "id": "glOB-eTG5avI"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4oY4qPz59qj",
        "outputId": "f42f1b85-f889-42d6-b2e8-5bfe7d953311"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9478666932284122"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distilled, quantized and converted to ONNX model"
      ],
      "metadata": {
        "id": "OvabMjB46Qh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime as ort\n",
        "\n",
        "# Load the quantized ONNX model\n",
        "quantized_model_path = \"/content/drive/MyDrive/iasa_nlp/student_model_quantized.onnx\"\n",
        "ort_session = ort.InferenceSession(quantized_model_path)\n",
        "\n",
        "distilled_predictions = []\n",
        "\n",
        "for batch in val_dataloader:\n",
        "    input_ids = batch['input_ids'].numpy()\n",
        "    # Run ONNX inference\n",
        "    ort_inputs = {ort_session.get_inputs()[0].name: input_ids}\n",
        "    ort_outputs = ort_session.run(None, ort_inputs)\n",
        "    logits = ort_outputs[0]\n",
        "    batch_predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    distilled_predictions.extend(batch_predictions.reshape(-1))\n",
        "\n",
        "# Filter and calculate metrics as before\n",
        "distilled_predictions = [pred for pred, label in zip(distilled_predictions, true_labels) if label != -100]\n",
        "\n",
        "distilled_precision, distilled_recall, distilled_f1, _ = precision_recall_fscore_support(true_labels, distilled_predictions, average='weighted')\n",
        "\n",
        "distilled_f1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxQfrWw45_Up",
        "outputId": "d27f6797-75e9-4912-fa9d-957cc106f428"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8667483818041759"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The F1 score went down for the optimizied model"
      ],
      "metadata": {
        "id": "_zUZTOSB6W85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Time measurement"
      ],
      "metadata": {
        "id": "18TgIb_L6oXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Measure inference time for the teacher model\n",
        "teacher_model.eval()\n",
        "start_time = time.time()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "        outputs = teacher_model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "teacher_inference_time = time.time() - start_time"
      ],
      "metadata": {
        "id": "QK9NyYht6qlF"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "for batch in val_dataloader:\n",
        "    input_ids = batch['input_ids'].numpy()\n",
        "\n",
        "    ort_inputs = {ort_session.get_inputs()[0].name: input_ids}\n",
        "    ort_outputs = ort_session.run(None, ort_inputs)\n",
        "\n",
        "distilled_inference_time = time.time() - start_time"
      ],
      "metadata": {
        "id": "tMzTvo1a6xA4"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Teacher Model Inference Time: {teacher_inference_time} seconds\")\n",
        "print(f\"Quantized Distilled Model Inference Time: {distilled_inference_time} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q69sfrpC60pQ",
        "outputId": "8d8694a2-c9a4-43cc-c54c-a201b9994d56"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher Model Inference Time: 6.094416379928589 seconds\n",
            "Quantized Distilled Model Inference Time: 2.5610992908477783 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimized model beats the original one in inference time"
      ],
      "metadata": {
        "id": "6g_-i3QS65Ai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CONCLUSION: I compared the original model an the model that was optimized, and the F1 score for optimized went down but can be better. Possible steps to improve the optimization:\n",
        "- check each step separately to understand at which point the accuracy decreased dramatically.\n",
        "- I tried converting to ONNX and then quantizing the model. Perhaps it would make sense to first apply PyTorch quantization and then convert to ONNX\n",
        "- Play with quantization, I simply tried convert to int8"
      ],
      "metadata": {
        "id": "J08Nfjyy7Ay1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FApkHhCKx9TD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NwfmgJaWyzFj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}